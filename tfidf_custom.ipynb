{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvLvmewlxLix"
   },
   "source": [
    "What does tf-idf mean?\n",
    "\n",
    "\n",
    "Tf-idf stands for <em>term frequency-inverse document frequency</em>, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.\n",
    "\n",
    "One of the simplest ranking functions is computed by summing the tf-idf for each query term; many more sophisticated ranking functions are variants of this simple model.\n",
    "\n",
    "Tf-idf can be successfully used for stop-words filtering in various subject fields including text summarization and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAcTjhO8xLiz"
   },
   "source": [
    "### 1. Custom TFIDF Vectorizer & compare its results with Sklearn:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnV82tg1xLi0"
   },
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bUsYm9wjxLi1"
   },
   "outputs": [],
   "source": [
    "#Collection of string documents\n",
    "\n",
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLwmFZfKxLi4"
   },
   "source": [
    "### SkLearn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Np4dfQOkxLi4"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "skl_output = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-7Om8YpYxLi6",
    "outputId": "0a3bd0f5-4424-4400-944f-4482a80bd799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dilip\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#sklearn feature names, sorted in alphabetic order by default.\n",
    "\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dTKplK96xLi-",
    "outputId": "53722fa2-6756-4aa0-f179-37b578bb6890"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n"
     ]
    }
   ],
   "source": [
    "#Here we will print the sklearn tfidf vectorizer idf values after applying the fit method\n",
    "\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-CTiWHygxLjA",
    "outputId": "8d5a9cde-2c29-4afe-f7b4-1547e88dba4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of sklearn tfidf vectorizer after applying transform method.\n",
    "\n",
    "skl_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bDKEpbA-xLjD",
    "outputId": "87dafd65-5313-443f-8c6e-1b05cc8c2543"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n"
     ]
    }
   ],
   "source": [
    "#sklearn tfidf values for first line of the above corpus.\n",
    "#output is a sparse matrix\n",
    "\n",
    "print(skl_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3QWo34hexLjF",
    "outputId": "cdc04e08-989f-4bdc-dd7f-f1c82a9f90be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "#sklearn tfidf values for first line of the above corpus.\n",
    "#here we are converting the sparse output matrix to dense matrix and printing it.\n",
    "#output is normalized using L2 normalization. sklearn does this by default.\n",
    "\n",
    "print(skl_output[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfIwx5LzxLjI"
   },
   "source": [
    "### custom implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HjuCcJwXxLjJ"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(dataset):\n",
    "    \"\"\"Return Vocabulary of Unique Words\"\"\"\n",
    "    unique_words = set()\n",
    "\n",
    "    #Check if the input is a list of sentences\n",
    "    if isinstance(dataset, list):\n",
    "        #Iterate through each sentence in the dataset\n",
    "        for row in dataset:\n",
    "            #Split the sentence into words\n",
    "            for word in row.split(\" \"):\n",
    "                #Ignore words with length less than 2\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                #Add unique words to the set\n",
    "                unique_words.add(word)\n",
    "\n",
    "        #Sort the unique words and create a vocabulary mapping\n",
    "        unique_words = sorted(list(unique_words))\n",
    "        vocab = {j: i for i, j in enumerate(unique_words)}\n",
    "        return vocab\n",
    "    else:\n",
    "        print(\"You need to pass a list of sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 0, 'document': 1, 'first': 2, 'is': 3, 'one': 4, 'second': 5, 'the': 6, 'third': 7, 'this': 8}\n"
     ]
    }
   ],
   "source": [
    "vocab=fit(corpus)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def idf(corpus, vocab):\n",
    "    \"\"\"Return Inverse Document Frequency values\"\"\"\n",
    "    idf_values = {}\n",
    "    total_documents = len(corpus)  #Total number of documents in the collection\n",
    "\n",
    "    #Iterate through each word in the vocabulary\n",
    "    for word in vocab.keys():\n",
    "        if len(word) < 2:\n",
    "            continue\n",
    "\n",
    "        #Count the number of documents containing the word\n",
    "        document_count = sum(1 for row in corpus if word in row)\n",
    "\n",
    "        #Calculate Inverse Document Frequency\n",
    "        idf_values[word] = 1 + (math.log((1 + total_documents) / (1 + document_count)))\n",
    "\n",
    "    return idf_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 1.916290731874155,\n",
       " 'document': 1.2231435513142097,\n",
       " 'first': 1.5108256237659907,\n",
       " 'is': 1.0,\n",
       " 'one': 1.916290731874155,\n",
       " 'second': 1.916290731874155,\n",
       " 'the': 1.0,\n",
       " 'third': 1.916290731874155,\n",
       " 'this': 1.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf(corpus,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def tf(corpus, vocab):\n",
    "    \"\"\"Return the Term Frequency values\"\"\"\n",
    "    tf_values = {}\n",
    "\n",
    "    #Check if the input is a list of documents\n",
    "    if isinstance(corpus, list):\n",
    "        #Iterate through each document in the corpus\n",
    "        for idx, row in enumerate(corpus):\n",
    "            #Calculate word frequency in the document\n",
    "            word_frequency = dict(Counter(row.split()))\n",
    "            total_words = sum(word_frequency.values())\n",
    "\n",
    "            #Iterate through each word in the vocabulary\n",
    "            for word in vocab.keys():\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "\n",
    "                #Calculate Term Frequency for each word\n",
    "                if word in word_frequency:\n",
    "                    tf_values[word] = word_frequency[word] / total_words\n",
    "                else:\n",
    "                    tf_values[word] = 0\n",
    "\n",
    "    return tf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(corpus, vocab):\n",
    "    \"\"\"Return TFIDF Values as a Sparse Matrix\"\"\"\n",
    "    rows = []\n",
    "    columns = []\n",
    "    values = []\n",
    "    tfidf = {}\n",
    "\n",
    "    #Check if the input is a list of documents\n",
    "    if isinstance(corpus, list):\n",
    "        #Iterate through each document in the corpus\n",
    "        for idx, row in enumerate(corpus):\n",
    "            lst = []\n",
    "            lst.append(row)\n",
    "\n",
    "            #Calculate TFIDF values for the document\n",
    "            tf_val = tf(lst, vocab)\n",
    "            idf_val = idf(corpus, vocab)\n",
    "\n",
    "            #Iterate through each word in the vocabulary\n",
    "            for word, value in vocab.items():\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "\n",
    "                #Calculate TFIDF for each word\n",
    "                tfidf[word] = tf_val[word] * idf_val[word]\n",
    "\n",
    "                #Add non-zero TFIDF values to the sparse matrix\n",
    "                if tfidf[word] != 0:\n",
    "                    rows.append(idx)\n",
    "                    columns.append(value)\n",
    "                    values.append(tfidf[word])\n",
    "\n",
    "        #Create a CSR matrix and normalize it\n",
    "        csr_mat = csr_matrix((values, (rows, columns)), shape=(len(corpus), len(vocab)))\n",
    "        l2_norm = normalize(csr_mat, norm='l2')\n",
    "\n",
    "        #information about the sparse matrix\n",
    "        print(l2_norm[0])\n",
    "        print(\"***************************************\")\n",
    "        print(l2_norm[0].toarray())\n",
    "        print(\"***************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.4697913855799205\n",
      "  (0, 2)\t0.580285823684436\n",
      "  (0, 3)\t0.3840852409148149\n",
      "  (0, 6)\t0.3840852409148149\n",
      "  (0, 8)\t0.3840852409148149\n",
      "***************************************\n",
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n",
      "***************************************\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#custom implementation output\n",
    "tf_idf=transform(corpus,vocab)\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n",
      "*****************************\n",
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "print(skl_output[0])\n",
    "print(\"*****************************\")\n",
    "print(skl_output[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Conclusion\n",
    " \n",
    " Both the OUTPUT are Same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_3_Instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
